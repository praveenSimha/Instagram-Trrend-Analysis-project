import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the dataset
df = pd.read_csv('instagram_data.csv', encoding='ISO-8859-1')

# Data Pre-Processing & Feature Selection
# Data Cleaning
df = df.fillna(0)  # Filling missing values

# Transforming categorical variables into numerical variables
from sklearn.preprocessing import LabelEncoder

if 'Caption' in df.columns:
    le_caption = LabelEncoder()
    df['Caption'] = le_caption.fit_transform(df['Caption'])
    
if 'Hashtags' in df.columns:
    le_hashtags = LabelEncoder()
    df['Hashtags'] = le_hashtags.fit_transform(df['Hashtags'])

# Data Visualization
# Bar Chart
plt.figure(figsize=(10, 6))
df['Likes'].value_counts().plot(kind='bar')
plt.title('Distribution of Likes')
plt.xlabel('Number of Likes')
plt.ylabel('Frequency')
plt.show()

# Heat Map
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Histogram
plt.figure(figsize=(10, 6))
df['Likes'].plot(kind='hist', bins=30)
plt.title('Likes Histogram')
plt.xlabel('Likes')
plt.ylabel('Frequency')
plt.show()

# Pie Chart
labels = df['Hashtags'].value_counts().index
sizes = df['Hashtags'].value_counts().values
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Hashtags Distribution')
plt.show()


# Splitting the data
X = df.drop(['Likes'], axis=1)
y = df['Likes']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluating the model
y_pred = rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

# Check if accuracy of model â‰¥ 75%
accuracy = rf_model.score(X_test, y_test)
print(f'Accuracy: {accuracy * 100:.2f}%')
if accuracy < 0.75:
    print("Accuracy less than 75%, consider changing the algorithm.")
else:
    print("Accuracy is satisfactory.")

# Visualize Actual vs Predicted Values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual Likes')
plt.ylabel('Predicted Likes')
plt.title('Actual vs Predicted Likes')
plt.show()

# Feature Importance
importances = rf_model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

# Build the Predictive Model (Already done with RandomForestRegressor)
# Deploy the model (Implementation depends on the platform used for deployment)

